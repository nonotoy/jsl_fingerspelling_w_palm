{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nonotoy/yubimoji_palm/blob/main/yubimoji_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM0CLLiIILm6"
      },
      "source": [
        "### Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqnvuVLiI3H5"
      },
      "source": [
        "#### 0_Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SQOyELtAtlx",
        "outputId": "7152a44e-d427-44d5-e592-ccd247c78ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.16.1 scikeras #keras torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pydot graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqrWH-leiaV6",
        "outputId": "959b64fe-ca72-44d8-a4fd-e56a0b9bbabf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "RsePkwVXIRZW"
      },
      "outputs": [],
      "source": [
        "#pip uninstall scikeras keras tensorflow -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rcNmFRJxIPNX"
      },
      "outputs": [],
      "source": [
        "##pip install scikeras==0.13.0 keras==3.2.1 tensorflow==2.16.1 scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vDZ9qtz3pj7"
      },
      "source": [
        "#### 1_Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1ApLNsynQu6",
        "outputId": "62dce787-4c3e-4849-d0fc-ad3d9b70d3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.4.2\n",
            "scikeras version: 0.13.0\n",
            "keras version: 3.3.2\n",
            "tensorflow version: 2.16.1\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "\n",
        "import scikeras\n",
        "print(\"scikeras version:\", scikeras.__version__)\n",
        "\n",
        "import keras\n",
        "print(\"keras version:\", keras.__version__)\n",
        "\n",
        "import tensorflow\n",
        "print(\"tensorflow version:\", tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPBnoFAvP8c4",
        "outputId": "2343bf94-c05c-4adf-ee6a-82ff22c5bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.16.1\n",
            "GPU available: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL3Aj-vs88a1",
        "outputId": "b2b4b555-2dec-4896-c370-b98d82c22e2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 12385778663794766570\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 21991653376\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17334220247690801415\n",
              " physical_device_desc: \"device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Bidirectional, Dropout, Conv1D, MaxPooling1D, BatchNormalization, Lambda\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egne7hcQtAXQ"
      },
      "source": [
        "#### 2_Common def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "_qip1Ebxs_jJ"
      },
      "outputs": [],
      "source": [
        "# Common parameters\n",
        "n_sequence = 15  # シーケンス長\n",
        "n_classes = 76 # 出力クラス (指文字) の数\n",
        "\n",
        "def setup_pathandparams(mode):\n",
        "\n",
        "    if mode == 0:\n",
        "        withPalm = True\n",
        "        palmNormlized = True\n",
        "\n",
        "        # 手掌長あり & 手掌長正規化\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_normalisedpalm_combined.csv'\n",
        "        model_save_path = '/content/drive/MyDrive/Colab Notebooks/JSL/gesture_classifier_0'\n",
        "\n",
        "        # Parameters\n",
        "        n_features = 41  # 特徴量の数 (ランドマークx, y 各20列 + 手掌長 1列)\n",
        "\n",
        "    elif mode == 1:\n",
        "        withPalm = True\n",
        "        palmNormlized = False\n",
        "\n",
        "        # 手掌長あり\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_palm_combined.csv'\n",
        "        model_save_path = '/content/drive/MyDrive/Colab Notebooks/JSL/gesture_classifier_1'\n",
        "\n",
        "        # Parameters\n",
        "        n_features = 41  # 特徴量の数 (ランドマークx, y 各20列 + 手掌長 1列)\n",
        "\n",
        "    elif mode == 2:\n",
        "        withPalm = False\n",
        "        palmNormlized = False\n",
        "\n",
        "        # 手掌長なし\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_combined.csv'\n",
        "        model_save_path = '/content/drive/MyDrive/Colab Notebooks/JSL/gesture_classifier_2'\n",
        "\n",
        "        # Parameters\n",
        "        n_features = 40  # 特徴量の数 (ランドマークx, y 各20列)\n",
        "\n",
        "    return withPalm, palmNormlized, dataset, model_save_path, n_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_oYAMBlKlCL"
      },
      "source": [
        "### Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOzZUW38JEh6"
      },
      "outputs": [],
      "source": [
        "def createLSTMModel():\n",
        "\n",
        "    # モデル\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(n_sequence, n_features)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        LSTM(512, return_sequences=False, kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # モデルのコンパイル\n",
        "    model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class EnsembleModel(tf.keras.layers.Layer):\n",
        "    def __init__(self, models):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.models = models\n",
        "\n",
        "    def call(self, inputs):\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            pred = model(inputs)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        ensemble_pred = tf.keras.backend.mean(tf.keras.backend.stack(predictions), axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(EnsembleModel, self).get_config()\n",
        "        config.update({\n",
        "            'models': [model.get_config() for model in self.models]\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        models = [tf.keras.Sequential.from_config(model_config) for model_config in config['models']]\n",
        "        return cls(models)\n",
        "\n",
        "\n",
        "def create_ensemble_model(models, input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    ensemble_layer = EnsembleModel(models)(input_layer)\n",
        "    ensemble_model = Model(inputs=input_layer, outputs=ensemble_layer)\n",
        "    return ensemble_model\n",
        "\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, input_shape, n_classes, num_models, epochs, batch_size):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        model = createLSTMModel()\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "\n",
        "def predict_ensemble(models, X_test):\n",
        "    predictions = []\n",
        "    for model in models:\n",
        "        pred = model.predict(X_test)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    ensemble_pred = np.mean(predictions, axis=0)\n",
        "    return np.argmax(ensemble_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "lTfaKeh9JUgu",
        "outputId": "309ebe7f-db23-4893-a1e0-c8d92ad4c63a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6f0016078f4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Ground truth label (1st cln)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0my_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# One-hot encoding of the GT label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "for mode in range(3):\n",
        "\n",
        "    withPalm, palmNormlized, dataset, model_save_path, n_features = setup_pathandparams(mode)\n",
        "\n",
        "    # Ground truth label (1st cln)\n",
        "    y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "\n",
        "    # One-hot encoding of the GT label\n",
        "    y_dataset = to_categorical(y_dataset, num_classes=n_classes)\n",
        "\n",
        "    # Training data (2nd cln to the last cln)\n",
        "    X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, n_features+1))) # (20730, 40) or ,41)\n",
        "\n",
        "    # サンプル数をシーケンス長で割り切れるように調整\n",
        "    total_samples = len(X_dataset)\n",
        "    max_samples = total_samples - total_samples % n_sequence\n",
        "\n",
        "    # Reshape\n",
        "    X_dataset = X_dataset[:max_samples]\n",
        "    y_dataset = y_dataset[:max_samples]\n",
        "    X_dataset = X_dataset.reshape(-1, n_sequence, n_features)\n",
        "    y_dataset = y_dataset.reshape(-1, n_sequence, n_classes)\n",
        "\n",
        "    #ensemble_model = create_ensemble_model((n_sequence, n_features), n_classes)\n",
        "\n",
        "    # K-fold cross validation\n",
        "    k = 5\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    print('withPalm: ', withPalm, '| palmNormlized: ', palmNormlized)\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(model_save_path, verbose=0, save_weights_only=False)\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=0)\n",
        "\n",
        "    for train_index, test_index in kf.split(X_dataset):\n",
        "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
        "        y_train, y_test = y_dataset[train_index], y_dataset[test_index]\n",
        "\n",
        "        y_train = np.argmax(y_train[:, -1, :], axis=1)\n",
        "        y_test = np.argmax(y_test[:, -1, :], axis=1)\n",
        "\n",
        "        models = train_ensemble_models(X_train, y_train, (n_sequence, n_features), n_classes, num_models=5, epochs=200, batch_size=128)\n",
        "        ensemble_model = create_ensemble_model(models, input_shape=(n_sequence, n_features))\n",
        "\n",
        "        y_pred = ensemble_model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        precision = precision_score(y_test, y_pred, average='macro')\n",
        "        recall = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    print(f'Precision: {np.mean(precision_scores):.4f} (+/- {np.std(precision_scores):.4f})')\n",
        "    print(f'Recall: {np.mean(recall_scores):.4f} (+/- {np.std(recall_scores):.4f})')\n",
        "    print(f'F1 Score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})')\n",
        "\n",
        "    ensemble_model.save(model_save_path + '.h5')\n",
        "    #ensemble_model.save(model_save_path + '.tflite')\n",
        "\n",
        "    print('--------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjD0o1xEFZ99"
      },
      "source": [
        "### No ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Hz-nABf2_sFg"
      },
      "outputs": [],
      "source": [
        "#from keras.regularizers import l1_l2, l2\n",
        "from tensorflow.keras.regularizers import l1_l2, l2\n",
        "\n",
        "def createLSTMModel():\n",
        "\n",
        "    regularizer = l2(0.001)\n",
        "\n",
        "    # ElasticNet正則化の設定\n",
        "    #l1_ratio = 0.05 # L1正則化の割合 (0 < l1_ratio < 1)\n",
        "    #regularizer = l1_l2(l1=0.001 * l1_ratio, l2=0.001 * (1-l1_ratio))\n",
        "\n",
        "    '''model = Sequential([\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(n_sequence, n_features)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        LSTM(512, return_sequences=False, kernel_regularizer=regularizer),\n",
        "        Dense(512, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(256, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])'''\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(n_sequence, n_features)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        #LSTM(2048, return_sequences=True, kernel_regularizer=regularizer),\n",
        "        Bidirectional(LSTM(1024, return_sequences=True, kernel_regularizer=regularizer)),\n",
        "        Bidirectional(LSTM(512, return_sequences=False, kernel_regularizer=regularizer)),\n",
        "        #LSTM(1024, return_sequences=True, kernel_regularizer=regularizer),\n",
        "        #LSTM(512, return_sequences=False, kernel_regularizer=regularizer),\n",
        "        #Dense(512, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        #Dropout(0.2),\n",
        "        Dense(256, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
        "        #BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "\n",
        "    '''model = Sequential([\n",
        "            Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer=regularizer, bias_regularizer=regularizer, input_shape=(n_sequence, n_features)),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            LSTM(512, return_sequences=True, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer),\n",
        "            LSTM(256, return_sequences=True, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer),\n",
        "            LSTM(128, return_sequences=False, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "            Dense(128, activation='relu', kernel_regularizer=regularizer, bias_regularizer=regularizer),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu', kernel_regularizer=regularizer, bias_regularizer=regularizer),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "            Dense(n_classes, activation='softmax')\n",
        "        ])'''\n",
        "\n",
        "    model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGEGdU61Fcor",
        "outputId": "964b0130-db3f-4cde-d6a3-1c4376f374fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "withPalm:  True | palmNormlized:  True\n"
          ]
        }
      ],
      "source": [
        "y_test_0 = ''\n",
        "y_test_1 = ''\n",
        "y_test_2 = ''\n",
        "X_test_0 = ''\n",
        "X_test_1 = ''\n",
        "X_test_2 = ''\n",
        "\n",
        "for mode in range(3):\n",
        "\n",
        "    withPalm, palmNormlized, dataset, model_save_path, n_features = setup_pathandparams(mode)\n",
        "\n",
        "    # Ground truth label (1st cln)\n",
        "    y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "\n",
        "    # One-hot encoding of the GT label\n",
        "    y_dataset = to_categorical(y_dataset, num_classes=n_classes)\n",
        "\n",
        "    # Training data (2nd cln to the last cln)\n",
        "    X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, n_features+1))) # (20730, 40) or ,41)\n",
        "\n",
        "    # サンプル数をシーケンス長で割り切れるように調整\n",
        "    max_samples = len(X_dataset) - len(X_dataset) % n_sequence\n",
        "\n",
        "    # Reshape\n",
        "    X_dataset = X_dataset[:max_samples]\n",
        "    y_dataset = y_dataset[:max_samples]\n",
        "    X_dataset = X_dataset.reshape(-1, n_sequence, n_features)\n",
        "    y_dataset = y_dataset.reshape(-1, n_sequence, n_classes)\n",
        "\n",
        "    print('withPalm: ', withPalm, '| palmNormlized: ', palmNormlized)\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(model_save_path + '_bilstm.keras', verbose=0, save_weights_only=False)\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.8, random_state=42)\n",
        "    y_train = np.argmax(y_train[:, -1, :], axis=1)\n",
        "    y_test = np.argmax(y_test[:, -1, :], axis=1)\n",
        "\n",
        "    if mode == 0:\n",
        "        X_test_0 = X_test\n",
        "        y_test_0 = y_test\n",
        "    elif mode == 1:\n",
        "        X_test_1 = X_test\n",
        "        y_test_1 = y_test\n",
        "    elif mode == 2:\n",
        "        X_test_2 = X_test\n",
        "        y_test_2 = y_test\n",
        "\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for i in range(10):\n",
        "\n",
        "        model = createLSTMModel()\n",
        "\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=1000,\n",
        "            batch_size=256,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[cp_callback, es_callback],\n",
        "            verbose=0)\n",
        "\n",
        "        _, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "\n",
        "    y_pred = best_model.predict(X_test, verbose=0)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    print('--------------------')\n",
        "\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    print(f'Accuracy:\\t{best_accuracy:.2f}')\n",
        "    print(f'Precision:\\t{precision:.2f}')\n",
        "    print(f'Recall:\\t\\t{recall:.2f}')\n",
        "    print(f'F1 Score:\\t{f1:.2f}')\n",
        "\n",
        "    best_model.save(model_save_path + '.h5', overwrite=True)\n",
        "    #best_model.save(new_model_save_path + '.h5', include_optimizer=False, overwrite=True)\n",
        "\n",
        "    print('--------------------')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "\n",
        "for mode in range(3):\n",
        "    # load model\n",
        "    model = load_model('/content/drive/MyDrive/Colab Notebooks/JSL/gesture_classifier_{}.h5'.format(mode))\n",
        "\n",
        "    # plot model\n",
        "    file = str('/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/report/model_{}.png'.format(mode))\n",
        "    plot_model(model=model, show_shapes=True, expand_nested=True, to_file=file)\n",
        "\n",
        "    # save svg\n",
        "    SVG(data=model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sIMow91Of4u",
        "outputId": "12a25f02-c1b8-406a-ec90-4eab79ef1e11"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = model.predict(X_test_0)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test_0 predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zr-MUr-oDT84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted_classes)\n",
        "precision = precision_score(y_test, predicted_classes, average='weighted')\n",
        "recall = recall_score(y_test, predicted_classes, average='weighted')\n",
        "f1 = f1_score(y_test, predicted_classes, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "6GKAg8qYDyaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtFm4A9lF937"
      },
      "source": [
        "### Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "X0lOLAwqXjLb",
        "outputId": "91bbbae0-9fc9-4fed-f73c-23b2ee258899"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-b97cd92bd810>:55: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
            "    * make sure the original data is stored as integers.\n",
            "    * use the `converters=` keyword argument.  If you only use\n",
            "      NumPy 1.23 or later, `converters=float` will normally work.\n",
            "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
            "      floating point and then convert it.  (On all NumPy versions.)\n",
            "  (Deprecated NumPy 1.23)\n",
            "  y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "withPalm: True | palmNormlized: True\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected hidden[0] size (2, 128, 512), got [1, 128, 512]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b97cd92bd810>\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{num_epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b97cd92bd810>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b97cd92bd810>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# the user believes he/she is passing in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    788\u001b[0m                            ):\n\u001b[1;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    791\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    792\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    257\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_weights_have_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 128, 512), got [1, 128, 512]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# モデルの定義\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # 重みの初期化\n",
        "        torch.nn.init.xavier_uniform_(self.lstm.weight_ih_l0)\n",
        "        torch.nn.init.xavier_uniform_(self.lstm.weight_hh_l0)\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.relu(self.bn1(self.fc1(out[:, -1, :])))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.bn2(self.fc2(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.bn3(self.fc3(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.bn4(self.fc4(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc5(out)\n",
        "        return out\n",
        "\n",
        "def prepare_data(dataset, n_sequence, n_features, n_classes):\n",
        "    y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "\n",
        "    # 目標ラベルの範囲を確認し、必要に応じて調整\n",
        "    #assert np.min(y_dataset) >= 0 and np.max(y_dataset) < n_classes, \"Invalid target label range\"\n",
        "\n",
        "    y_dataset = torch.from_numpy(y_dataset).long()\n",
        "\n",
        "    X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, n_features+1)))\n",
        "\n",
        "    total_samples = len(X_dataset)\n",
        "    max_samples = total_samples - total_samples % n_sequence\n",
        "\n",
        "    X_dataset = X_dataset[:max_samples]\n",
        "    y_dataset = y_dataset[:max_samples]\n",
        "    X_dataset = X_dataset.reshape(-1, n_sequence, n_features)\n",
        "\n",
        "    return X_dataset, y_dataset\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clipping\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    return running_loss / len(dataloader), precision, recall, f1\n",
        "\n",
        "\n",
        "hidden_size = 512\n",
        "n_sequence = 15\n",
        "n_classes = 76\n",
        "num_layers = 2\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# デバイスの設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = 'cpu'\n",
        "\n",
        "for mode in range(3):\n",
        "    if mode == 0:\n",
        "        withPalm = True\n",
        "        palmNormlized = True\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_normalisedpalm_combined.csv'\n",
        "        model_save_path = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/yubimoji/palm_normalised_model/gesture_classifier'\n",
        "        n_features = 41\n",
        "    elif mode == 1:\n",
        "        withPalm = True\n",
        "        palmNormlized = False\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_palm_combined.csv'\n",
        "        model_save_path = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/yubimoji/palm_model/gesture_classifier'\n",
        "        n_features = 41\n",
        "    elif mode == 2:\n",
        "        withPalm = False\n",
        "        palmNormlized = False\n",
        "        dataset = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/point_history/normalised__point_history_combined.csv'\n",
        "        model_save_path = '/content/drive/Othercomputers/My Mac/Documents/2_study/1_修士/3_副研究/yubimoji/base_model/gesture_classifier'\n",
        "        n_features = 40\n",
        "\n",
        "    X_dataset, y_dataset = prepare_data(dataset, n_sequence, n_features, n_classes)\n",
        "\n",
        "    k = 5\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    print('withPalm:', withPalm, '| palmNormlized:', palmNormlized)\n",
        "\n",
        "    for train_index, test_index in kf.split(X_dataset):\n",
        "        X_train, X_test = X_dataset[train_index], X_dataset[test_index]\n",
        "        y_train, y_test = y_dataset[train_index], y_dataset[test_index]\n",
        "\n",
        "        train_dataset = TensorDataset(torch.from_numpy(X_train).float(), y_train)\n",
        "        test_dataset = TensorDataset(torch.from_numpy(X_test).float(), y_test)\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model = LSTMModel(n_features, hidden_size, num_layers, n_classes).to(device)\n",
        "        #model.config.use_cache = False\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "            train_loss = train_model(model, train_dataloader, criterion, optimizer, device)\n",
        "            test_loss, precision, recall, f1 = evaluate_model(model, test_dataloader, criterion, device)\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    print(f'Precision: {np.mean(precision_scores):.4f} (+/- {np.std(precision_scores):.4f})')\n",
        "    print(f'Recall: {np.mean(recall_scores):.4f} (+/- {np.std(recall_scores):.4f})')\n",
        "    print(f'F1 Score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})')\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path + '.pth')\n",
        "\n",
        "    print('--------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iM0CLLiIILm6",
        "Egne7hcQtAXQ",
        "y_oYAMBlKlCL",
        "YtFm4A9lF937"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "164P_-hKd3W9SjEpqp0PkZX3z0wC6YUus",
      "authorship_tag": "ABX9TyOGMqKmkG8m4X7cVVFFDT+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}